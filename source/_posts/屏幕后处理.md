---
title: 屏幕后处理
date: 2022-03-04 10:19:05
tags: Post Processing
categories: Unity Shader
math: true
---

## 建立一个基本的屏幕后处理脚本系统 ##

屏幕后处理，通常指的是在渲染完整个场景得到屏幕图像后，再对这个图像进行一系列操作，实现各种屏幕特效。使用这种技术，可以为游戏画面添加更多的艺术效果，例如景深（Depth of Field）、运动模糊（Motion Blur）等。

因此，想要实现屏幕后处理的基础在于得到渲染后的屏幕图像，即抓取屏幕，而 Unity 为我们提供了这样一个方便的接口——**OnRenderImage 函数**。它的函数声明如下：

```csharp
MonoBehaviour.OnRenderImage(RenderTexture src, RenderTexture dest)
```

当我们在脚本中声明此函数后，Unity 会把当前渲染得到的图像存储在第一个参数对应的源渲染纹理中，通过函数中的一系列操作后，再把目标渲染纹理，即第二个参数对应的渲染纹理显示到屏幕上。在 OnRenderImage 函数中，我们通常是利用 **Graphics.Blit 函数**来完成对渲染纹理的处理。它有 3 种函数声明：

```csharp
public static void Blit(Texture src, RenderTexture dest);
public static void Blit(Texture src, RenderTexture dest, Material mat, int pass = -1);
public static void Blit(Texture src, Material mat, int pass = -1);
```

其中，参数 src 对应了源纹理，在屏幕后处理技术中，这个参数通常就是当前屏幕的渲染纹理或是上一步处理后得到的渲染纹理。参数 dest 是目标渲染纹理，如果它的值为 null 就会直接将结果显示在屏幕上。参数 mat 是我们使用的材质，这个材质使用的 Unity Shader 将会进行各种屏幕后处理操作，而 src 纹理将会被传递给 Shader 中名为 _MainTex 的纹理属性。参数 pass 的默认值为 -1，表示将会依次调用 Shader 内的所有 Pass。否则，只会调用给定索引的 Pass。

在默认情况下，OnRenderImage 函数会在所有的不透明和透明的 Pass 执行完毕后被调用，以便对场景中所有游戏对象都产生影响。但有时，我们希望在不透明的 Pass（即渲染队列小于等于 2500 的 Pass，内置的 Background、Geometry 和 AlphaTest 渲染队列均在此范围内）执行完毕后立即调动 OnRenderImage 函数，从而不对透明物体产生任何影响。此时，我们可以在 OnRenderImage 函数前添加 ImageEffectOpaque 属性来实现这样的目的。

要在 Unity 中实现屏幕后处理效果，过程通常如下：我们首先需要在摄像机中添加一个用于屏幕后处理的脚本。在这个脚本中，我们会实现 OnRenderImage 函数来获取当前屏幕的渲染纹理。然后，再调用 Graphics.Blit 函数使用特定的 Unity Shader 来对当前图像进行处理，再把返回的渲染纹理显示到屏幕上。对于一些复杂的屏幕特效，我们可能需要多次调用 Graphics.Blit 函数来对上一步的输出结果进行下一步处理。

但是，在进行屏幕后处理之前，我们需要检查一系列条件是否满足，例如当前平台是否支持渲染纹理和屏幕特效，是否支持当前使用的 Unity Shader 等。为此，我们创建了一个用于屏幕后处理效果的基类，在实现各种屏幕特效时，我们只需要继承自该基类，再实现派生类中不同的操作即可。PostEffectsBase.cs 的主要代码如下。

```csharp
using UnityEngine;
using System.Collections;

[ExecuteInEditMode]
[RequireComponent (typeof(Camera))]
public class PostEffectsBase : MonoBehaviour {

	// Called when start
	protected void CheckResources() {
		bool isSupported = CheckSupport();
		
		if (isSupported == false) {
			NotSupported();
		}
	}

	// Called in CheckResources to check support on this platform
	protected bool CheckSupport() {
		if (SystemInfo.supportsImageEffects == false || SystemInfo.supportsRenderTextures == false) {
			Debug.LogWarning("This platform does not support image effects or render textures.");
			return false;
		}
		
		return true;
	}

	// Called when the platform doesn't support this effect
	protected void NotSupported() {
		enabled = false;
	}
	
	protected void Start() {
		CheckResources();
	}

	// Called when need to create the material used by this effect
	protected Material CheckShaderAndCreateMaterial(Shader shader, Material material) {
		if (shader == null) {
			return null;
		}
		
		if (shader.isSupported && material && material.shader == shader)
			return material;
		
		if (!shader.isSupported) {
			return null;
		}
		else {
			material = new Material(shader);
			material.hideFlags = HideFlags.DontSave;
			if (material)
				return material;
			else 
				return null;
		}
	}
}
```

CheckShaderAndCreateMaterial 函数接受两个参数，第一个参数指定了该特效需要使用的 Shader，第二个参数则是用于后期处理的材质。该函数首先检查 Shader 的可用性，检查通过后就返回一个使用了该 Shader 的材质，否则返回 null。

## 调整屏幕的亮度、饱和度和对比度 ##

首先编写 BrightnessSaturationAndContrast.cs 脚本。

```csharp
using UnityEngine;
using System.Collections;

public class BrightnessSaturationAndContrast : PostEffectsBase {

	public Shader briSatConShader;
	private Material briSatConMaterial;
	public Material material {  
		get {
			briSatConMaterial = CheckShaderAndCreateMaterial(briSatConShader, briSatConMaterial);
			return briSatConMaterial;
		}  
	}

	[Range(0.0f, 3.0f)]
	public float brightness = 1.0f;

	[Range(0.0f, 3.0f)]
	public float saturation = 1.0f;

	[Range(0.0f, 3.0f)]
	public float contrast = 1.0f;

	void OnRenderImage(RenderTexture src, RenderTexture dest) {
		if (material != null) {
			material.SetFloat("_Brightness", brightness);
			material.SetFloat("_Saturation", saturation);
			material.SetFloat("_Contrast", contrast);

			Graphics.Blit(src, dest, material);
		} else {
			Graphics.Blit(src, dest);
		}
	}
}
```

每当 OnRenderImage 函数被调用时，它会检查材质是否可用。如果可用，就把参数传递给材质，再调用 Graphics.Blit 进行处理；否则，直接把原图像显示到屏幕上，不做任何处理。

```shaderlab
Shader "Custom/Chapter12/Chapter12-BrightnessSaturationAndContrast"
{
    
    Properties {
		_MainTex ("Base (RGB)", 2D) = "white" {}
		// _Brightness ("Brightness", Float) = 1
		// _Saturation("Saturation", Float) = 1
		// _Contrast("Contrast", Float) = 1
	}
	SubShader {
		Pass {  
			ZTest Always Cull Off ZWrite Off
			
			CGPROGRAM  
			#pragma vertex vert  
			#pragma fragment frag  
			  
			#include "UnityCG.cginc"  
			  
			sampler2D _MainTex;  
			half _Brightness;
			half _Saturation;
			half _Contrast;
			  
			struct v2f {
				float4 pos : SV_POSITION;
				half2 uv: TEXCOORD0;
			};
			  
			v2f vert(appdata_img v) {
				v2f o;
				
				o.pos = UnityObjectToClipPos(v.vertex);
				
				o.uv = v.texcoord;
						 
				return o;
			}
		
			fixed4 frag(v2f i) : SV_Target {
				fixed4 renderTex = tex2D(_MainTex, i.uv);  
				  
				// Apply brightness
				fixed3 finalColor = renderTex.rgb * _Brightness;
				
				// Apply saturation
				fixed luminance = 0.2125 * renderTex.r + 0.7154 * renderTex.g + 0.0721 * renderTex.b;
				fixed3 luminanceColor = fixed3(luminance, luminance, luminance);
				finalColor = lerp(luminanceColor, finalColor, _Saturation);
				
				// Apply contrast
				fixed3 avgColor = fixed3(0.5, 0.5, 0.5);
				finalColor = lerp(avgColor, finalColor, _Contrast);
				
				return fixed4(finalColor, renderTex.a);  
			}  
			  
			ENDCG
		}  
	}
	
	Fallback Off
}

```

我们提到 Graphics.Blit(src, dest, material) 将把第一个参数传递给 Shader 中名为 _MainTex 的属性，因此我们必须声明一个名为 _MainTex 的纹理属性。除此之外，我们还声明了用于调整亮度、饱和度和对比度的属性。这些值将会由脚本传递而得。事实上，我们可以省略 Properties 中的属性声明，Properties 中声明的属性仅仅是为了显示在材质面板中，但对于屏幕特效来说，它们使用的材质都是临时创建的，我们也不需要在材质面板上调整参数，而是直接从脚本传递给 Unity Shader。

```shaderlab
ZTest Always Cull Off ZWrite Off
```
屏幕后处理实际上是在场景中绘制了一个与屏幕同宽同高的四边形面片，为了防止它对其他物体产生影响，我们需要设置相关的渲染状态，在这里我们关闭了深度写入，是为了防止它“挡住”在其后面被渲染的物体。例如，如果当前的 OnRenderImage 函数在所有不透明的 Pass 执行完毕后立即被调用，不关闭深度写入就会影响后面透明的 Pass 的渲染。这些状态设置可以认为是用于屏幕后处理的 Shader 的“标配”。

首先，我们得到对原屏幕图像（存储在 _MainTex 中)的采样结果 renderTex。然后利用 _Brightness 属性来调整亮度。亮度的调整非常简单，我们只需要把原颜色乘以亮度系数 _Brightness 即可。然后，我们计算该像素对应的亮度值（luminance），这是通过对每个颜色乘以一个特定的系数再相加得到的。我们使用该亮度值创建了一个饱和度为 0 的颜色值，并使用 _Saturation 属性在其和上一步得到的颜色之间进行插值，从而得到希望的饱和度颜色。对比度的处理类似，我们首先创建一个对比度为 0 的颜色值（各分量均为 0.5），再使用 _Contrast 属性在其和上一步得到的颜色之间进行插值，从而得到最终的处理结果。

![调整屏幕的亮度、饱和度和对比度](/posts_image/Post_Processing/Post_Processing_1.png "调整屏幕的亮度、饱和度和对比度")

## 边缘检测 ##

边缘检测的原理是利用一些边缘检测算子对图像进行**卷积（convolution）**操作，我们首先来了解什么是卷积。

### 什么是卷积 ###

在图像处理中，卷积操作指的就是使用一个**卷积核（kernel）**对一张图像中的每个像素进行一系列操作。卷积核通常是一个四方形网格结构（例如 2 $\times$ 2、3 $\times$ 3 的方形区域），该区域内每个方格都有一个权重值。当对图像中的某个像素进行卷积时，我们会把卷积核的中心放置于该像素上，如下图所示，翻转核之后再依次计算核中每个元素和其覆盖的图像像素值的乘积并求和，得到的结果就是该位置的新像素值。

![卷积核与卷积](/posts_image/Post_Processing/Post_Processing_2.png "卷积核与卷积")

这样的计算过程虽然简单，但可以实现很多常见的图像处理效果，例如图像模糊、边缘检测等。例如，如果我们想要对图像进行均值模糊，可以使用一个 3 $\times$ 3 的卷积核，核内每个元素的值均为 1/9。

### 常见的边缘检测算子 ###

卷积操作的神奇之处在于选择的卷积核。那么，用于边缘检测的卷积核（也被称为边缘检测算子）应该是什么样子的？在回答这个问题之前，我们可以首先回想一下边到底是如何形成的。如果相邻像素之间存在差别明显的颜色、亮度、纹理等属性，我们就会认为它们之间应该有一条边界。这种相邻像素之间的差值可以用**梯度（gradient）**来表示，可以想像得到，边缘处的梯度绝对值会比较大。基于这样的理解，有几种不同的边缘检测算子被先后提出来。

![3 种常见的边缘检测算子](/posts_image/Post_Processing/Post_Processing_3.png "3 种常见的边缘检测算子")

3 种常见的边缘检测算子如上图所示，它们都包含了两个方向的卷积核，分别用于检测水平方向和竖直方向上的边缘信息。在进行边缘检测时，我们需要对每个像素分别进行一次卷积计算，得到两个方向上的梯度值 $G_x$ 和 $G_y$，而整体的梯度可按下面的公式计算而得：

$$
G = \sqrt{G_x^2 + G_y^2}
$$

由于上述计算包含了开根号操作，出于性能的考虑，我们有时会使用绝对值操作来代替开根号操作：

$$
G = |G_x| + |G_y|
$$

当得到梯度 $G$ 后，我们就可以据此来判断哪些像素对应了边缘（梯度值越大，越有可能是边缘点）。

### 实现 ###

我们首先来编写 EdgeDetection 脚本。

```csharp
using UnityEngine;
using System.Collections;

public class EdgeDetection : PostEffectsBase {

	public Shader edgeDetectShader;
	private Material edgeDetectMaterial = null;
	public Material material {  
		get {
			edgeDetectMaterial = CheckShaderAndCreateMaterial(edgeDetectShader, edgeDetectMaterial);
			return edgeDetectMaterial;
		}  
	}

	[Range(0.0f, 1.0f)]
	public float edgesOnly = 0.0f;

	public Color edgeColor = Color.black;
	
	public Color backgroundColor = Color.white;

	void OnRenderImage (RenderTexture src, RenderTexture dest) {
		if (material != null) {
			material.SetFloat("_EdgeOnly", edgesOnly);
			material.SetColor("_EdgeColor", edgeColor);
			material.SetColor("_BackgroundColor", backgroundColor);

			Graphics.Blit(src, dest, material);
		} else {
			Graphics.Blit(src, dest);
		}
	}
}

```

当 edgesOnly 值为 0 时，边缘将会叠加在原渲染图像上；当 edgesOnly 值为 1 时，则会只显示边缘，不显示原渲染图像。其中，背景颜色由 backgroundColor 指定，边缘颜色由 edgeColor 指定。

```shaderlab
Shader "Custom/Chapter12/Chapter12-EdgeDetection"
{
    Properties
    {
        _MainTex ("Base (RGB)", 2D) = "white" {}
        _EdgeOnly ("Edge Only", Float) = 1.0
        _EdgeColor ("Edge Color", Color) = (0, 0, 0, 1)
        _BackgroundColor ("Background Color", Color) = (1, 1, 1, 1)
    }
    SubShader
    {
        Pass {
            ZTest Always Cull Off ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment fragSobel
            #include "UnityCG.cginc"

            sampler2D _MainTex;
            half4 _MainTex_TexelSize;
            fixed _EdgeOnly;
            fixed4 _EdgeColor;
            fixed4 _BackgroundColor;

            struct v2f {
                float4 pos : SV_POSITION;
                half2 uv[9] : TEXCOORD0; 
            };

            fixed luminance(fixed4 color) {
                return 0.2125 * color.r + 0.7154 * color.g + 0.0721 * color.b;
            }

            half Sobel(v2f i) {
                const half Gx[9] = {-1, -2, -1 ,0, 0, 0, 1, 2, 1};
                const half Gy[9] = {-1, 0, 1, -2, 0, 2, -1, 0, 1};
                half texColor;
                half edgeX = 0;
                half edgeY = 0;
                for(int it = 0; it < 9; it++){
                    texColor = luminance(tex2D(_MainTex, i.uv[it]));
                    edgeX += texColor * Gx[it];
                    edgeY += texColor * Gy[it];
                }
                half edge = 1 - abs(edgeX) - abs(edgeY);
                return edge;
            }

            v2f vert(appdata_img v) {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                half2 uv = v.texcoord;

                o.uv[0] = uv + _MainTex_TexelSize.xy * half2(-1, -1);
                o.uv[1] = uv + _MainTex_TexelSize.xy * half2(0, -1);
                o.uv[2] = uv + _MainTex_TexelSize.xy * half2(1, -1);
                o.uv[3] = uv + _MainTex_TexelSize.xy * half2(-1, 0);
                o.uv[4] = uv + _MainTex_TexelSize.xy * half2(0, 0);
                o.uv[5] = uv + _MainTex_TexelSize.xy * half2(1, 0);
                o.uv[6] = uv + _MainTex_TexelSize.xy * half2(-1, 1);
                o.uv[7] = uv + _MainTex_TexelSize.xy * half2(0, 1);
                o.uv[8] = uv + _MainTex_TexelSize.xy * half2(1, 1);

                return o;
            }

            fixed4 fragSobel(v2f i) : SV_TARGET {
                half edge = Sobel(i);
                fixed4 withEdgeColor = lerp(_EdgeColor, tex2D(_MainTex, i.uv[4]), edge);
                fixed4 onlyEdgeColor = lerp(_EdgeColor, _BackgroundColor, edge);
                return lerp(withEdgeColor, onlyEdgeColor, _EdgeOnly);
            }
            
            ENDCG
        }
    }
    FallBack Off
}

```
我们声明了一个变量 _MainTex_TexelSize，xxx_TexelSize 是 Unity 为我们提供的访问 xxx 纹理对应的每个纹素的大小。由于卷积需要对相邻区域内的纹理进行采样，因此我们需要利用 _MainTex_TexelSize 来计算各个相邻区域的纹理坐标。

我们在 v2f 结构体中定义了一个维数为 9 的纹理数组，对应了使用 Sobel 算子采样时需要的 9 个邻域纹理坐标。通过把计算采样纹理坐标的代码从片元着色器中转移到顶点着色器中，可以减少运算，提高性能。由于从顶点着色器到片元着色器到插值是线性的，因此这样的转移并不会影响纹理坐标的计算结果。

我们调用 Sobel 函数计算当前像素的梯度值 edge，并利用该值分别计算了背景为原图和纯色下的颜色值，然后利用 _EdgeOnly 在两者之间插值得到最终的像素值。Sobel 函数利用 Sobel 算子对原图进行边缘检测。在 Sobel 函数中，我们定义了水平方向和竖直方向使用的卷积核 $G_x$ 和 $G_y$。接着，我们依次对 9 个像素进行采样，计算它们的亮度值，再与卷积核 $G_x$ 和 $G_y$ 中对应的权重相乘后，叠加到各自的梯度值上。最后，我们从 1 中减去水平方向和竖直方向的梯度值的绝对值，得到 edge。edge 值越小，表明该位置越可能是一个边缘点。至此，边缘检测过程结束。

需要注意的是，本次实现的边缘检测仅仅利用了屏幕的颜色信息，而在实际应用中，物体的纹理、阴影等信息均会影响边缘检测的结果，使得结果包含许多非预期的描边。为了得到更加准确的边缘信息，我们往往会在屏幕的深度纹理和法线纹理上进行边缘检测。

![边缘检测结果](/posts_image/Post_Processing/Post_Processing_4.png "边缘检测结果")

![只显示边缘的效果](/posts_image/Post_Processing/Post_Processing_5.png "只显示边缘的效果")

## 高斯模糊 ##

模糊的实现有很多方法，例如均值模糊和中值模糊。均值模糊同样使用了卷积操作，它使用的卷积核中的各个元素值都相等，且相加等于 1，也就是说，卷积后得到的像素值是其邻域内各个像素值的平均值。而中值模糊则是选择邻域内对所有像素排序后的中值替换掉原颜色。一个更高级的模糊方法是高斯模糊。

### 高斯滤波 ###

高斯模糊同样利用了卷积计算，它使用的卷积核名为高斯核。高斯核是一个正方形大小的滤波核，其中每个元素的计算都是基于下面的高斯方程：



